{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HandsOn Week 13\n",
    "Welcome to HandsOn Week 13. In this HandsOn, you will try to play with spark streaming where the data is from a Kafka producer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1\n",
    "In this milestone, you need to setup Zookeeper and Kafka in your computer/VM you use to run Spark. See [here](https://towardsdatascience.com/kafka-python-explained-in-10-lines-of-code-800e3e07dad1) for one of the tutorials and [here](https://tecadmin.net/install-apache-kafka-ubuntu/) for additional one to ubuntu users. Then, you also need to install kafka-python by ```pip install kafka-python```.<br><br>\n",
    "In this milestone, you only need to run ```producer_variance.py``` and ```consumer_variance.py``` (these two code files are already provided inside the zip file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screenshot your ```consumer_variance.py``` output, and put in this cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Output Consumer\" align=\"left\" src=\"Week13_M1.png\" alt=\"Drawing\" style=\"width: 850px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2\n",
    "After making sure that the message is published by ```producer_variance.py``` and successfully consumed by ```consumer_variance.py``` in the topic of ```variance``` in the Milestone 1 above, then, you are ready for Milestone 2.<br>\n",
    "\n",
    "In Milestone 2, you need to implement ```calculate_variance``` function with the formula --> $variance = \\frac{\\sum_{i=1}^{N}x_i^2}{N}-(\\frac{\\sum_{i=1}^{N}x_i}{N})^2$. This function will be used to calculate variance for each window operation to the streaming data, and the variance is \"**accumulative/global**\" value up to current stream data. For example, in the first window, we have data ```1,2,3```, then the variance is the variance of ```1,2,3```. Let's say we have streaming data of ```4,5,6``` in the second window, thus the variance in this second window is the variance of ```1,2,3,4,5,6```, and so on for the following windows.<br>\n",
    "\n",
    "The ```calculate_variance``` function will return a DStream (RDD) with a format of ```('sum_x_square:', sum_x_square_value, 'sum_x:', sum_x_value, 'n:', n_value, 'var:', variance_value)``` where ```sum_x_square_value```$=\\sum_{i=1}^{N}x_i^2$, ```sum_x_value```$=\\sum_{i=1}^{N}x_i$ and ```n```$=N$. Note that $x_i=$ i-th of individual stream data, and $N=$ the number of individual stream data -count- up to i-th data.\n",
    "\n",
    "**Important:** In order to stream from Kafka producer to Spark Streaming, you need to download [spark-streaming-kafka-0-8-assembly_2.11-2.4.5.jar](https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.5) from maven repository (adjust with your Spark version), and put this jar file to ```your_spark_folder/jars```. For VM provided by the class, ```spark_folder``` is in ```/home/bigdata/spark-2.4.5-bin-hadoop2.7```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "sudo systemctl start zookeeper\n",
    "sudo systemctl start kafka\n",
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:19\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273, 'sum_x:', 63, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:21\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 91, 'sum_x:', 21, 'n:', 6, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:23\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 182, 'sum_x:', 42, 'n:', 12, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:25\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 182, 'sum_x:', 42, 'n:', 12, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:27\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 182, 'sum_x:', 42, 'n:', 12, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:29\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273, 'sum_x:', 63, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:31\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273, 'sum_x:', 63, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:33\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273, 'sum_x:', 63, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:35\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 364, 'sum_x:', 84, 'n:', 24, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:37\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273, 'sum_x:', 63, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:39\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 364, 'sum_x:', 84, 'n:', 24, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:41\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 273, 'sum_x:', 63, 'n:', 18, 'var:', 2.916666666666666)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-04-14 03:58:43\n",
      "-------------------------------------------\n",
      "('sum_x_square:', 364, 'sum_x:', 84, 'n:', 24, 'var:', 2.916666666666666)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-138ab6b371e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "KAFKA_TOPIC = \"variance\"\n",
    "BOOTSTRAP_SERVER = \"localhost:9092\"\n",
    "\n",
    "ssc = StreamingContext(sc,1) #stream each one second\n",
    "ssc.checkpoint(\"./checkpoint\")\n",
    "lines = KafkaUtils.createDirectStream(ssc, [KAFKA_TOPIC],\n",
    "                                      {\"metadata.broker.list\": BOOTSTRAP_SERVER})\n",
    "\n",
    "def calculate_variance(lines, window_length = 2, sliding_interval = 2):\n",
    "    \"\"\"\n",
    "    Function to calculate \"accumulated/global variance\" in each window operation\n",
    "    Params:\n",
    "        lines: Spark DStream defined above (in this jupyter cell)\n",
    "        window_length: length of window in windowing operation\n",
    "        sliding_interval: sliding interval for the window operation\n",
    "    Return:\n",
    "        result: DStream (RDD) of variance result with \n",
    "                format --> ('sum_x_square:', sum_x_square_value, 'sum_x:', sum_x_value, 'n:', n_value, 'var:', variance_value)\n",
    "                Example:   ('sum_x_square:', 182.0, 'sum_x:', 42.0, 'n:', 12.0, 'var:', 2.916666666666666)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Realize this function here. Note that you are not allowed to modify any code other than this function.    \n",
    "    temp = lines.window(window_length, sliding_interval).map(lambda x: (x[1].split(' ')))\n",
    "    \n",
    "    sum_x_square_value = temp.flatMap(lambda x: (int(i) ** 2 for i in x)).reduce(lambda x, y: x + y)\n",
    "    sum_x_value = temp.flatMap(lambda x: (int(i) for i in x)).reduce(lambda x, y: x + y)\n",
    "    n_value = temp.flatMap(lambda x: (1 for i in x)).reduce(lambda x, y: x + y)\n",
    "    \n",
    "    temp1 = sum_x_square_value.map(lambda x: (1, x))\n",
    "    temp2 = sum_x_value.map(lambda x: (1, x))\n",
    "    temp3 = n_value.map(lambda x: (1, x))    \n",
    "    temp4 = temp1.join(temp2).join(temp3)\n",
    "    variance_value = temp4.map(lambda x: (x[1][0][0] / x[1][1] - (x[1][0][1] / x[1][1]) ** 2))\n",
    "    \n",
    "    result1 = sum_x_square_value.map(lambda x: (1, ('sum_x_square:', x)))\n",
    "    result2 = sum_x_value.map(lambda x: (1, ('sum_x:', x)))\n",
    "    result3 = n_value.map(lambda x: (1, ('n:', x)))\n",
    "    result4 = variance_value.map(lambda x: (1, ('var:', x)))\n",
    "    result5 = result1.join(result2).join(result3).join(result4)\n",
    "    result = result5.map(lambda x: ((x[1][0][0][0][0], x[1][0][0][0][1], x[1][0][0][1][0], x[1][0][0][1][1], x[1][0][1][0], x[1][0][1][1], x[1][1][0], x[1][1][1])))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# run the function\n",
    "result = calculate_variance(lines, window_length=2, sliding_interval=2)\n",
    "# Print\n",
    "result.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Archive this ipynb file and the screenshot image needed in the Milestone 1 into zip file with a format of: ```HandsOnWeek13_NIM_FullName.zip```, and submit to the course portal.\n",
    "\n",
    "**Note**: make sure in the Milestone 2, the cell has its output, but not too many streams (you can save this ipynb file approximatelly in the range of 4-20 window operations)\n",
    "\n",
    "Enjoy..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
