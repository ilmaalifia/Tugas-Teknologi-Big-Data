{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HandsOn Week 14\n",
    "Welcome to handsOn week 14: graph in distributed system, using GraphFrames. To use GraphFrames, you need to run ```pyspark --packages graphframes:graphframes:0.8.0-spark2.4-s_2.11``` (adjust with your Spark version. That script uses Spark 2.4.x, the one used in our VM, and it will download the graphframes package automatically, thus, make sure you have internet connection when launching the script).\n",
    "\n",
    "Note: you can use any Spark & GraphFrames API (without building from-the-scratch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The Dataset and Build The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+----------+\n",
      "|              id| latitude|longitude|population|\n",
      "+----------------+---------+---------+----------+\n",
      "|       Amsterdam| 52.37919| 4.899431|    821752|\n",
      "|         Utrecht|52.092876|  5.10448|    334176|\n",
      "|        Den Haag|52.078663| 4.288788|    514861|\n",
      "|       Immingham| 53.61239| -0.22219|      9642|\n",
      "|       Doncaster| 53.52285| -1.13116|    302400|\n",
      "|Hoek van Holland|  51.9775|  4.13333|      9382|\n",
      "|      Felixstowe| 51.96375|   1.3511|     23689|\n",
      "|         Ipswich| 52.05917|  1.15545|    133384|\n",
      "|      Colchester| 51.88921|  0.90421|    104390|\n",
      "|          London|51.509865|-0.118092|   8787892|\n",
      "|       Rotterdam|  51.9225|  4.47917|    623652|\n",
      "|           Gouda| 52.01667|  4.70833|     70939|\n",
      "+----------------+---------+---------+----------+\n",
      "\n",
      "+----------------+----------------+------------+-----+\n",
      "|             src|             dst|relationship| cost|\n",
      "+----------------+----------------+------------+-----+\n",
      "|       Amsterdam|         Utrecht|       EROAD| 46.0|\n",
      "|       Amsterdam|        Den Haag|       EROAD| 59.0|\n",
      "|        Den Haag|       Rotterdam|       EROAD| 26.0|\n",
      "|       Amsterdam|       Immingham|       EROAD|369.0|\n",
      "|       Immingham|       Doncaster|       EROAD| 74.0|\n",
      "|       Doncaster|          London|       EROAD|277.0|\n",
      "|Hoek van Holland|        Den Haag|       EROAD| 27.0|\n",
      "|      Felixstowe|Hoek van Holland|       EROAD|207.0|\n",
      "|         Ipswich|      Felixstowe|       EROAD| 22.0|\n",
      "|      Colchester|         Ipswich|       EROAD| 32.0|\n",
      "|          London|      Colchester|       EROAD|106.0|\n",
      "|           Gouda|       Rotterdam|       EROAD| 25.0|\n",
      "|           Gouda|         Utrecht|       EROAD| 35.0|\n",
      "|        Den Haag|           Gouda|       EROAD| 32.0|\n",
      "|Hoek van Holland|       Rotterdam|       EROAD| 33.0|\n",
      "|         Utrecht|       Amsterdam|       EROAD| 46.0|\n",
      "|        Den Haag|       Amsterdam|       EROAD| 59.0|\n",
      "|       Rotterdam|        Den Haag|       EROAD| 26.0|\n",
      "|       Immingham|       Amsterdam|       EROAD|369.0|\n",
      "|       Doncaster|       Immingham|       EROAD| 74.0|\n",
      "+----------------+----------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from graphframes import *\n",
    "\n",
    "def create_transport_graph(nodes_filePath, edges_filePath):\n",
    "    node_fields = [\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"latitude\", FloatType(), True),\n",
    "        StructField(\"longitude\", FloatType(), True),\n",
    "        StructField(\"population\", IntegerType(), True)\n",
    "    ]\n",
    "    nodes = spark.read.csv(nodes_filePath, header=True,\n",
    "                           schema=StructType(node_fields))\n",
    "    edge_fields = [\n",
    "        StructField(\"src\", StringType(), True),\n",
    "        StructField(\"dst\", StringType(), True),\n",
    "        StructField(\"relationship\", StringType(), True),\n",
    "        StructField(\"cost\", FloatType(), True)\n",
    "    ]\n",
    "    rels = spark.read.csv(edges_filePath, header=True,\n",
    "                         schema=StructType(edge_fields))\n",
    "    reversed_rels = (rels.withColumn(\"newSrc\", rels.dst)\n",
    "                     .withColumn(\"newDst\", rels.src)\n",
    "                     .drop(\"dst\", \"src\")\n",
    "                     .withColumnRenamed(\"newSrc\", \"src\")\n",
    "                     .withColumnRenamed(\"newDst\", \"dst\")\n",
    "                     .select(\"src\", \"dst\", \"relationship\", \"cost\"))\n",
    "\n",
    "    relationships = rels.union(reversed_rels)\n",
    "    \n",
    "    return GraphFrame(nodes, relationships)\n",
    "\n",
    "g = create_transport_graph(\"./transport-nodes.csv\", \"./transport-relationships.csv\")\n",
    "g.vertices.show()\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 01\n",
    "1. Find source-destination airport pair/pairs with maximum cost\n",
    "2. Find source-destination airport pair/pairs with minimum cost\n",
    "3. Calculate the average cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \n",
      "+---------+---------+-----+\n",
      "|      src|      dst| cost|\n",
      "+---------+---------+-----+\n",
      "|Amsterdam|Immingham|369.0|\n",
      "|Immingham|Amsterdam|369.0|\n",
      "+---------+---------+-----+\n",
      "\n",
      "2. \n",
      "+----------+----------+----+\n",
      "|       src|       dst|cost|\n",
      "+----------+----------+----+\n",
      "|   Ipswich|Felixstowe|22.0|\n",
      "|Felixstowe|   Ipswich|22.0|\n",
      "+----------+----------+----+\n",
      "\n",
      "3. \n",
      "+-----------------+\n",
      "|        avg(cost)|\n",
      "+-----------------+\n",
      "|91.33333333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "print('1. ')\n",
    "g.edges.createOrReplaceTempView('g_edges')\n",
    "spark.sql(\"SELECT src, dst, cost FROM g_edges WHERE cost == (SELECT max(cost) FROM g_edges)\").show()\n",
    "\n",
    "print('2. ')\n",
    "spark.sql(\"SELECT src, dst, cost FROM g_edges WHERE cost == (SELECT min(cost) FROM g_edges)\").show()\n",
    "\n",
    "print('3. ')\n",
    "spark.sql(\"SELECT avg(cost) FROM g_edges\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 02\n",
    "1. Find flight routes that have no direct connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \n",
      "+--------------------+--------------------+\n",
      "|              city_1|              city_2|\n",
      "+--------------------+--------------------+\n",
      "|[Felixstowe, 51.9...|[Gouda, 52.01667,...|\n",
      "|[London, 51.50986...|[Felixstowe, 51.9...|\n",
      "|[Immingham, 53.61...|[Hoek van Holland...|\n",
      "|[Rotterdam, 51.92...|[London, 51.50986...|\n",
      "|[Immingham, 53.61...|[Gouda, 52.01667,...|\n",
      "|[Utrecht, 52.0928...|[Colchester, 51.8...|\n",
      "|[Amsterdam, 52.37...|[Hoek van Holland...|\n",
      "|[Amsterdam, 52.37...|[Colchester, 51.8...|\n",
      "|[Den Haag, 52.078...|[London, 51.50986...|\n",
      "|[Felixstowe, 51.9...|[Doncaster, 53.52...|\n",
      "|[Gouda, 52.01667,...|[Doncaster, 53.52...|\n",
      "|[Hoek van Holland...|[Utrecht, 52.0928...|\n",
      "|[Doncaster, 53.52...|[Colchester, 51.8...|\n",
      "|[Felixstowe, 51.9...|[Utrecht, 52.0928...|\n",
      "|[Colchester, 51.8...|[Amsterdam, 52.37...|\n",
      "|[Hoek van Holland...|[Doncaster, 53.52...|\n",
      "|[London, 51.50986...|[Den Haag, 52.078...|\n",
      "|[Felixstowe, 51.9...|[Immingham, 53.61...|\n",
      "|[Hoek van Holland...|[London, 51.50986...|\n",
      "|[Utrecht, 52.0928...|[Ipswich, 52.0591...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "print('1. ')\n",
    "g.find(\"!(city_1)-[]->(city_2)\").filter(\"city_1.id != city_2.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 03\n",
    "1. Find the most important airport (you can use any measurement to judge the importance level of airports, and please describe why you choose it -the measurement-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \n",
      "+--------+------+\n",
      "|      id|degree|\n",
      "+--------+------+\n",
      "|Den Haag|     8|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "print('1. ')\n",
    "\n",
    "g.degrees.createOrReplaceTempView('g_degrees')\n",
    "spark.sql(\"SELECT id, degree FROM g_degrees WHERE degree = (SELECT max(degree) FROM g_degrees)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree of importance used is based on how busy the airport, it is depicted by how many routes does the airport served. So, the code finds the maximum value of degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 04\n",
    "1. Find the sortest path based on \"node\" -path with fewest nodes- from Amsterdam to London "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  e0|                  e1|                  e2|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[Amsterdam, Immin...|[Immingham, Donca...|[Doncaster, Londo...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "print('1. ')\n",
    "g.bfs(\"id = 'Amsterdam'\", \"id = 'London'\").select('e0', 'e1', 'e2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "1. Find the sortest path based on \"cost value\" -see 'cost' column in the edge/relationship dataframe - from Amsterdam to London "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------------------------------------------------------------------+\n",
      "|id    |distance|path                                                                            |\n",
      "+------+--------+--------------------------------------------------------------------------------+\n",
      "|London|453.0   |[Amsterdam, Den Haag, Hoek van Holland, Felixstowe, Ipswich, Colchester, London]|\n",
      "+------+--------+--------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code taken from oreilly.com\n",
    "\n",
    "from graphframes.lib import AggregateMessages as AM\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "add_path_udf = F.udf(lambda path, id: path + [id], ArrayType(StringType()))\n",
    "\n",
    "def shortest_path(g, origin, destination, column_name=\"cost\"):\n",
    "    if g.vertices.filter(g.vertices.id == destination).count() == 0:\n",
    "        return (spark.createDataFrame(sc.emptyRDD(), g.vertices.schema)\n",
    "                .withColumn(\"path\", F.array()))\n",
    "\n",
    "    vertices = (g.vertices.withColumn(\"visited\", F.lit(False))\n",
    "                .withColumn(\"distance\", F.when(g.vertices[\"id\"] == origin, 0)\n",
    "                            .otherwise(float(\"inf\")))\n",
    "                .withColumn(\"path\", F.array()))\n",
    "    cached_vertices = AM.getCachedDataFrame(vertices)\n",
    "    g2 = GraphFrame(cached_vertices, g.edges)\n",
    "\n",
    "    while g2.vertices.filter('visited == False').first():\n",
    "        current_node_id = g2.vertices.filter('visited == False').sort(\"distance\").first().id\n",
    "\n",
    "        msg_distance = AM.edge[column_name] + AM.src['distance']\n",
    "        msg_path = add_path_udf(AM.src[\"path\"], AM.src[\"id\"])\n",
    "        msg_for_dst = F.when(AM.src['id'] == current_node_id,\n",
    "                             F.struct(msg_distance, msg_path))\n",
    "        new_distances = g2.aggregateMessages(F.min(AM.msg).alias(\"aggMess\"),\n",
    "                                             sendToDst=msg_for_dst)\n",
    "\n",
    "        new_visited_col = F.when(\n",
    "            g2.vertices.visited | (g2.vertices.id == current_node_id), True).otherwise(False)\n",
    "        new_distance_col = F.when(new_distances[\"aggMess\"].isNotNull() &\n",
    "                                 (new_distances.aggMess[\"col1\"]\n",
    "                                 < g2.vertices.distance),\n",
    "                                 new_distances.aggMess[\"col1\"]).otherwise(g2.vertices.distance)\n",
    "        new_path_col = F.when(new_distances[\"aggMess\"].isNotNull() &\n",
    "                       (new_distances.aggMess[\"col1\"]\n",
    "                       < g2.vertices.distance), new_distances.aggMess[\"col2\"]\n",
    "                       .cast(\"array<string>\")).otherwise(g2.vertices.path)\n",
    "\n",
    "        new_vertices = (g2.vertices.join(new_distances, on=\"id\",\n",
    "                                         how=\"left_outer\")\n",
    "                        .drop(new_distances[\"id\"])\n",
    "                        .withColumn(\"visited\", new_visited_col)\n",
    "                        .withColumn(\"newDistance\", new_distance_col)\n",
    "                        .withColumn(\"newPath\", new_path_col)\n",
    "                        .drop(\"aggMess\", \"distance\", \"path\")\n",
    "                        .withColumnRenamed('newDistance', 'distance')\n",
    "                        .withColumnRenamed('newPath', 'path'))\n",
    "        cached_new_vertices = AM.getCachedDataFrame(new_vertices)\n",
    "        g2 = GraphFrame(cached_new_vertices, g2.edges)\n",
    "        if g2.vertices.filter(g2.vertices.id == destination).first().visited:\n",
    "            return (g2.vertices.filter(g2.vertices.id == destination)\n",
    "                    .withColumn(\"newPath\", add_path_udf(\"path\", \"id\"))\n",
    "                    .drop(\"visited\", \"path\")\n",
    "                    .withColumnRenamed(\"newPath\", \"path\"))\n",
    "    return (spark.createDataFrame(sc.emptyRDD(), g.vertices.schema)\n",
    "            .withColumn(\"path\", F.array()))\n",
    "\n",
    "result = shortest_path(g, \"Amsterdam\", \"London\", \"cost\")\n",
    "result.select(\"id\", \"distance\", \"path\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Submit this ```ipynb``` file to the course portal, with format: ```HandsOnWeek14_NIM_NamaLengkap.ipynb```. Make sure when submitting this file, each code cell has the outputs (not blank)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
